{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "BycbBAIthxeD",
        "3pVBNSrwh1-8"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f649db9a6e584324b222286bd146dfa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_041fc71e0f184d19abe605c8810a9d1b",
              "IPY_MODEL_f78d08057a2d43a1a136d72d7d1b16c9",
              "IPY_MODEL_2159a40ce5ba483586ccb48f8f48fbf0"
            ],
            "layout": "IPY_MODEL_35e4c648d9fc4f63999ed727fbc6944f"
          }
        },
        "041fc71e0f184d19abe605c8810a9d1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_845aca44db3f49f8b70b8647c773ed3c",
            "placeholder": "​",
            "style": "IPY_MODEL_209a6d41063b4f06a9e81e92e371c791",
            "value": "Map: 100%"
          }
        },
        "f78d08057a2d43a1a136d72d7d1b16c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8751eec7cf6249bcbb6cb70a5361f38f",
            "max": 10570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f08255590005490ea690d07398216a38",
            "value": 10570
          }
        },
        "2159a40ce5ba483586ccb48f8f48fbf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f7829dccf83415e95728f46aa5a9662",
            "placeholder": "​",
            "style": "IPY_MODEL_1370cf4c1205443f8ccf01268c7dae8c",
            "value": " 10570/10570 [00:04&lt;00:00, 2123.80 examples/s]"
          }
        },
        "35e4c648d9fc4f63999ed727fbc6944f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "845aca44db3f49f8b70b8647c773ed3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "209a6d41063b4f06a9e81e92e371c791": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8751eec7cf6249bcbb6cb70a5361f38f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f08255590005490ea690d07398216a38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5f7829dccf83415e95728f46aa5a9662": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1370cf4c1205443f8ccf01268c7dae8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADyr_U5ss-M1"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets\n",
        "!pip install -U accelerate\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "raw_datasets = load_dataset(\"squad\")\n",
        "\n",
        "model_checkpoint = \"distilbert-base-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "HySj3j-PlVeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenisation"
      ],
      "metadata": {
        "id": "BycbBAIthxeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_answer_token_idx(\n",
        "    ctx_start,\n",
        "    ctx_end,\n",
        "    ans_start_char,\n",
        "    ans_end_char,\n",
        "    offset):\n",
        "  \"\"\"\n",
        "  Determines, if the answer is fully included in the context window,\n",
        "  and returns the start and end token indices of the answer\n",
        "  in the context window, if so.\n",
        "\n",
        "  ctx_start: Start-token index of the context window.\n",
        "  ctx_end: End-token index of the context window.\n",
        "  ans_start_char: First char of the answer in context.\n",
        "  ans_end_char: Last char of the answer in context.\n",
        "  offset: List of tuples, containing the start and end char of every\n",
        "          token of the input.\n",
        "  \"\"\"\n",
        "\n",
        "  start_idx = 0\n",
        "  end_idx = 0\n",
        "\n",
        "  if offset[ctx_start][0] > ans_start_char or offset[ctx_end][1] < ans_end_char:\n",
        "    pass\n",
        "    # nothing else to do\n",
        "  else:\n",
        "    i = ctx_start\n",
        "    for start_end_char in offset[ctx_start:]:\n",
        "      start, end = start_end_char\n",
        "      if start == ans_start_char:\n",
        "        start_idx = i\n",
        "      if end == ans_end_char:\n",
        "        end_idx = i\n",
        "        break\n",
        "\n",
        "      i += 1\n",
        "  return start_idx, end_idx"
      ],
      "metadata": {
        "id": "lQre9ToMlyVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# google used thesse values in their experiments\n",
        "max_length = 384\n",
        "stride = 128\n",
        "\n",
        "def tokenize_fn_train(batch):\n",
        "  \"\"\"\n",
        "  Takes a question+context-pair input sample and splits\n",
        "  them into multiple question+context_window input samples\n",
        "  with max_length tokens. Additionaly it determines the token\n",
        "  of the answers in each context window, if the answer is fully\n",
        "  included.\n",
        "\n",
        "  batch: Batch of input samples.\n",
        "  \"\"\"\n",
        "  # some questions have leading and/or trailing whitespace\n",
        "  questions = [q.strip() for q in batch[\"question\"]]\n",
        "\n",
        "\n",
        "  inputs = tokenizer(\n",
        "    questions,\n",
        "    batch[\"context\"],\n",
        "    max_length=max_length,\n",
        "    # truncate only context, since this gets splitted\n",
        "    truncation=\"only_second\",\n",
        "    stride=stride,\n",
        "    return_overflowing_tokens=True,\n",
        "    return_offsets_mapping=True,\n",
        "    # all input samples have max_length tokens\n",
        "    padding=\"max_length\",\n",
        "  )\n",
        "\n",
        "  offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "  orig_sample_idxs = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "  answers = batch['answers']\n",
        "  start_idxs, end_idxs = [], []\n",
        "\n",
        "  for i, offset in enumerate(offset_mapping):\n",
        "    sample_idx = orig_sample_idxs[i]\n",
        "    answer = answers[sample_idx]\n",
        "\n",
        "    ans_start_char = answer['answer_start'][0]\n",
        "    ans_end_char = ans_start_char + len(answer['text'][0])\n",
        "\n",
        "    sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "    # find start + end of context (first 1 and last 1)\n",
        "    ctx_start = sequence_ids.index(1)\n",
        "    ctx_end = len(sequence_ids) - sequence_ids[::-1].index(1) - 1\n",
        "\n",
        "    start_idx, end_idx = find_answer_token_idx(\n",
        "      ctx_start,\n",
        "      ctx_end,\n",
        "      ans_start_char,\n",
        "      ans_end_char,\n",
        "      offset)\n",
        "\n",
        "    start_idxs.append(start_idx)\n",
        "    end_idxs.append(end_idx)\n",
        "\n",
        "  inputs[\"start_positions\"] = start_idxs\n",
        "  inputs[\"end_positions\"] = end_idxs\n",
        "  return inputs"
      ],
      "metadata": {
        "id": "T2Qst2SiouNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = raw_datasets[\"train\"].map(\n",
        "  tokenize_fn_train,\n",
        "  batched=True,\n",
        "  remove_columns=raw_datasets[\"train\"].column_names,\n",
        ")\n",
        "len(raw_datasets[\"train\"]), len(train_dataset)"
      ],
      "metadata": {
        "id": "RF8E0ISiSq_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize the validation set differently\n",
        "# since the metrics gets computed from the string answers\n",
        "# also: overwrite offset_mapping with Nones in place of question\n",
        "def tokenize_fn_validation(batch):\n",
        "  questions = [q.strip() for q in batch[\"question\"]]\n",
        "\n",
        "  inputs = tokenizer(\n",
        "    questions,\n",
        "    batch[\"context\"],\n",
        "    max_length=max_length,\n",
        "    truncation=\"only_second\",\n",
        "    stride=stride,\n",
        "    return_overflowing_tokens=True,\n",
        "    return_offsets_mapping=True,\n",
        "    padding=\"max_length\",\n",
        "  )\n",
        "\n",
        "  orig_sample_idxs = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "  sample_ids = []\n",
        "\n",
        "  # rewrite offset mapping by replacing question tuples with None\n",
        "  # this will be helpful later on when we compute metrics\n",
        "  for i in range(len(inputs[\"input_ids\"])):\n",
        "    sample_idx = orig_sample_idxs[i]\n",
        "    sample_ids.append(batch['id'][sample_idx])\n",
        "\n",
        "    sequence_ids = inputs.sequence_ids(i)\n",
        "    offset = inputs[\"offset_mapping\"][i]\n",
        "    inputs[\"offset_mapping\"][i] = [\n",
        "      x if sequence_ids[j] == 1 else None for j, x in enumerate(offset)]\n",
        "\n",
        "  inputs['sample_id'] = sample_ids\n",
        "  return inputs"
      ],
      "metadata": {
        "id": "D7lHAuysVP_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_dataset = raw_datasets[\"validation\"].map(\n",
        "  tokenize_fn_validation,\n",
        "  batched=True,\n",
        "    remove_columns=raw_datasets[\"validation\"].column_names,\n",
        ")\n",
        "len(raw_datasets[\"validation\"]), len(validation_dataset)"
      ],
      "metadata": {
        "id": "Ijyao62UcXbQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "f649db9a6e584324b222286bd146dfa9",
            "041fc71e0f184d19abe605c8810a9d1b",
            "f78d08057a2d43a1a136d72d7d1b16c9",
            "2159a40ce5ba483586ccb48f8f48fbf0",
            "35e4c648d9fc4f63999ed727fbc6944f",
            "845aca44db3f49f8b70b8647c773ed3c",
            "209a6d41063b4f06a9e81e92e371c791",
            "8751eec7cf6249bcbb6cb70a5361f38f",
            "f08255590005490ea690d07398216a38",
            "5f7829dccf83415e95728f46aa5a9662",
            "1370cf4c1205443f8ccf01268c7dae8c"
          ]
        },
        "outputId": "0cbe8a70-0498-4c40-f71d-90eb7ec62519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f649db9a6e584324b222286bd146dfa9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10570, 10822)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics"
      ],
      "metadata": {
        "id": "3pVBNSrwh1-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "metric = load_metric(\"squad\")"
      ],
      "metadata": {
        "id": "6VRGvW-qh66C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# needed format:\n",
        "predicted_answers = [\n",
        "  {'id': '1', 'prediction_text': 'Albert Einstein'},\n",
        "  {'id': '2', 'prediction_text': 'physicist'},\n",
        "  {'id': '3', 'prediction_text': 'general relativity'},\n",
        "]\n",
        "true_answers = [\n",
        "  {'id': '1', 'answers': {'text': ['Albert Einstein'], 'answer_start': [100]}},\n",
        "  {'id': '2', 'answers': {'text': ['physicist'], 'answer_start': [100]}},\n",
        "  {'id': '3', 'answers': {'text': ['special relativity'], 'answer_start': [100]}},\n",
        "]\n",
        "\n",
        "# exact match only counts the samples, where answer is fully correct.\n",
        "# f1 also takes partial correct answers into account\n",
        "metric.compute(predictions=predicted_answers, references=true_answers)"
      ],
      "metadata": {
        "id": "w7_84z4Di_nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.autonotebook import tqdm\n",
        "\n",
        "# only search in the tokens with the n_largest probabilitys\n",
        "n_largest = 20\n",
        "# max length of answer\n",
        "max_answer_length = 30\n",
        "\n",
        "\n",
        "def compute_metrics(start_logits, end_logits, processed_dataset, orig_dataset):\n",
        "  \"\"\"\n",
        "  Determines the answer based on the model output.\n",
        "  For this, the start and end tokens with the highest probabilities are used.\n",
        "  The corresponding tokens must meet following conditions, to build a valid answer:\n",
        "\n",
        "  1. start-token Timestep < end-token Timestep\n",
        "\n",
        "  2. start and end-token must be included in the context_window part of the input\n",
        "\n",
        "  3. answer must be less then 30 tokens long\n",
        "\n",
        "  start_logits: (batch_size, sequence_length), logits for the start-token\n",
        "  end_logits: (batch_size, sequence_length, logits for the end-tokens\n",
        "  processed_dataset: tokenized and context-windowed dataset\n",
        "  orig_datasset: unprocessed dataset\n",
        "  \"\"\"\n",
        "  # maps every question+context_window pair to the id of the original sample\n",
        "  sample_id2idxs = {}\n",
        "  for i, id_ in enumerate(processed_dataset['sample_id']):\n",
        "    if id_ not in sample_id2idxs:\n",
        "      sample_id2idxs[id_] = [i]\n",
        "    else:\n",
        "      sample_id2idxs[id_].append(i)\n",
        "\n",
        "  predicted_answers = []\n",
        "  for sample in tqdm(orig_dataset):\n",
        "\n",
        "    sample_id = sample['id']\n",
        "    context = sample['context']\n",
        "\n",
        "    # initialisation\n",
        "    best_score = float('-inf')\n",
        "    best_answer = None\n",
        "\n",
        "    # loop over every question+context_window-pair,\n",
        "    # to get the highest probability tokens over all these pairs,\n",
        "    # for the corresponding original question+context-pair\n",
        "    for idx in sample_id2idxs[sample_id]:\n",
        "      start_logit = start_logits[idx] # (seq_len,) vector\n",
        "      end_logit = end_logits[idx] # (seq_len,) vector\n",
        "\n",
        "      # do NOT do the reverse indexing: ['offset_mapping'][idx], much slower\n",
        "      offsets = processed_dataset[idx]['offset_mapping']\n",
        "\n",
        "      # get the highest probability tokens\n",
        "      start_indices = (-start_logit).argsort()\n",
        "      end_indices = (-end_logit).argsort()\n",
        "      for start_idx in start_indices[:n_largest]:\n",
        "        for end_idx in end_indices[:n_largest]:\n",
        "\n",
        "          # skip answers not contained in context window\n",
        "          if offsets[start_idx] is None or offsets[end_idx] is None:\n",
        "            continue\n",
        "\n",
        "          # skip answers where end < start\n",
        "          if end_idx < start_idx:\n",
        "            continue\n",
        "\n",
        "          # skip answers that are too long\n",
        "          if end_idx - start_idx + 1 > max_answer_length:\n",
        "            continue\n",
        "\n",
        "          # see cell down under for score calculation\n",
        "          score = start_logit[start_idx] + end_logit[end_idx]\n",
        "          if score > best_score:\n",
        "            best_score = score\n",
        "\n",
        "            # find positions of start and end characters\n",
        "            first_ch = offsets[start_idx][0]\n",
        "            last_ch = offsets[end_idx][1]\n",
        "\n",
        "            best_answer = context[first_ch:last_ch]\n",
        "\n",
        "    # save best answer in needed format\n",
        "    predicted_answers.append({'id': sample_id, 'prediction_text': best_answer})\n",
        "\n",
        "  # save labels in needed format\n",
        "  true_answers = [\n",
        "    {'id': x['id'], 'answers': x['answers']} for x in orig_dataset\n",
        "  ]\n",
        "  return metric.compute(predictions=predicted_answers, references=true_answers)"
      ],
      "metadata": {
        "id": "9fg8fOIakE-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## From logits to probability scores"
      ],
      "metadata": {
        "id": "fsFpJ-FTgiwf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\n",
        "P(s_i)...probability ~that ~start ~token ~of ~answer ~is ~on ~timestep ~i  \\\\  \n",
        "P(e_j)...probability ~that ~end ~token ~of ~answer ~is ~on ~timestep ~j\n",
        "$"
      ],
      "metadata": {
        "id": "_Td06zQWlHsv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Goal, with the independence assumption on the distributions:\n",
        "\n",
        "$\n",
        "P(s_i, e_j)=P(s_i)P(e_j) ~~ -> ~~ max\n",
        "$"
      ],
      "metadata": {
        "id": "odSQlhHwlMWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "numerically stable computation via logits:"
      ],
      "metadata": {
        "id": "o_ExBH-Nlohl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\n",
        "l(s_i) = logit(s_i) ~~~~~ s_i...~start ~token ~on ~timestep ~i  \\\\  \n",
        "l(e_j) = logit(e_j) ~~~~~ e_j...~end ~token ~on ~timestep ~j\n",
        "$"
      ],
      "metadata": {
        "id": "OPZZsfWSiluR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$P(s_i)=\\frac{e^{l(s_i)}}{\\sum_{n=1}^Te^{l(s_i)}}=\\frac{e^{l(s_i)}}{Z_s}$\n",
        "\n",
        "$P(e_j)=\\frac{e^{l(e_j)}}{\\sum_{n=1}^Te^{l(e_j)}}=\\frac{e^{l(e_j)}}{Z_e}$"
      ],
      "metadata": {
        "id": "erX_gPPZmTAj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\n",
        "\\underset{i,j}{argmax} ~ P(s_i)P(e_j)  \\\\\n",
        "= \\underset{i,j}{argmax} ~ \\frac{e^{l(s_i)}}{Z_s}\\frac{e^{l(e_j)}}{Z_e}\n",
        "$"
      ],
      "metadata": {
        "id": "XVx5nAYtuLdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\n",
        "Z_s ~and ~Z_e ~are ~independent ~from ~i ~and ~j\n",
        "$"
      ],
      "metadata": {
        "id": "lYHL6NGlwtAY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\n",
        "\\underset{i,j}{argmax} ~ \\frac{e^{l(s_i)}}{Z_s}\\frac{e^{l(e_j)}}{Z_e} ~=~ \\underset{i,j}{argmax} ~ e^{l(s_i)}e^{l(e_j)} ~=~\n",
        "\\underset{i,j}{argmax} ~ log(e^{l(s_i)}e^{l(e_j)}) \\\\\n",
        "\\\\\n",
        "=\\underset{i,j}{argmax} ~ l(s_i)+l(e_j)\n",
        "$"
      ],
      "metadata": {
        "id": "_yKPlUmPxH6g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and evaluate"
      ],
      "metadata": {
        "id": "liIU42_gCKES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "from google.colab import files\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    \"finetuned-squad\",\n",
        "    evaluation_strategy=\"no\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,\n",
        ")"
      ],
      "metadata": {
        "id": "KK8LueE8COIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    # train_dataset=train_dataset.shuffle(seed=42).select(range(1_000)),\n",
        "    eval_dataset=validation_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "8x1UdhsuCjJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_output = trainer.predict(validation_dataset)\n",
        "\n",
        "predictions, _, _ = trainer_output\n",
        "\n",
        "start_logits, end_logits = predictions\n",
        "\n",
        "compute_metrics(\n",
        "    start_logits,\n",
        "    end_logits,\n",
        "    validation_dataset, # processed\n",
        "    raw_datasets[\"validation\"], # orig\n",
        ")"
      ],
      "metadata": {
        "id": "8LaLizB3bHcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving and uploading to S3"
      ],
      "metadata": {
        "id": "R2ueJ8fRa2gL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model('trained_model')"
      ],
      "metadata": {
        "id": "PsNZcXAwa-M2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for direct download\n",
        "!zip -r trained_model.zip trained_model\n",
        "files.download('trained_model.zip')"
      ],
      "metadata": {
        "id": "ultadijSZB0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "\n",
        "s3 = boto3.client('s3')\n",
        "\n",
        "bucket_name = 'extractive-qa-models-v1'\n",
        "\n",
        "def create_bucket(bucket_name):\n",
        "    s3.create_bucket(Bucket=bucket_name)\n",
        "    print(\"Bucket is created\")\n",
        "\n",
        "create_bucket(bucket_name)"
      ],
      "metadata": {
        "id": "bNQsW6jHSWpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def upload_directory(directory_path, s3_prefix):\n",
        "    for root, dirs, files in os.walk(directory_path):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file).replace(\"\\\\\", \"/\")\n",
        "            relpath = os.path.relpath(file_path, directory_path)\n",
        "            s3_key = os.path.join(s3_prefix, relpath).replace(\"\\\\\", \"/\")\n",
        "\n",
        "            s3.upload_file(file_path, bucket_name, s3_key)\n",
        "\n",
        "upload_directory('/content/trained_model', 'models')"
      ],
      "metadata": {
        "id": "d5eO6L5VbQ-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def list_objects():\n",
        "    response = s3.list_objects_v2(Bucket=bucket_name)\n",
        "    for obj in response['Contents']:\n",
        "        print(obj['Key'])\n",
        "\n",
        "list_objects()"
      ],
      "metadata": {
        "id": "kLx1OR0DbpgN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}